# Project: CallAssist-LLM
# Objective:
Integrate a locally hosted LLM (Ollama + Mistral 7B) into the existing CallAssist application to enhance reasoning and tone personalization of system responses.

---

## 1. Overview

The previous version of CallAssist produced direct factual responses using a static Q&A JSON and fuzzy matching.  
This version adds a **reasoning and phrasing layer** powered by an LLM running locally through Ollama.

The Whisper ASR, intent detection, and Q&A matcher remain unchanged.  
Only the **response generation pipeline** changes — the LLM will interpret customer context and matched answers, then output a conversational, professional, “Chris Voss-style” message using `persona.txt`.

---

## 2. System Architecture Update

Audio (VOIP channel)
↓
Whisper → text
↓
Intent parser / multi-question splitter
↓
Q&A matcher → factual answers
↓
LLM reasoning layer (Ollama Mistral 7B)
↓
Formatted response → UI

yaml
Copy code

- Whisper handles speech-to-text.  
- The matcher retrieves factual answers from `qa_script.json`.  
- Mistral 7B refines tone, combines multiple answers, and personalizes wording.

---

## 3. Ollama Integration Requirements

### Installation (developer side)
1. Install Ollama: https://ollama.ai/download  
2. Pull the model:
ollama pull mistral

bash
Copy code
3. Verify local API endpoint:
http://localhost:11434

python
Copy code

### Runtime dependency
- No internet access required once model is pulled.
- Runs as a lightweight local service; ensure Ollama daemon is active before app launch.

---

## 4. LLM Integration Plan

### New module: `llm.py`

Responsibilities:
- Load `persona.txt` (single read at startup).
- Accept input from main pipeline: `customer_text` + `matched_answers[]`.
- Format an adaptive prompt for the LLM.
- Call Ollama API via `requests.post`.
- Return the generated response to the UI thread.

Example core function:

```python
import requests, json

def generate_response(persona: str, customer_text: str, answers: list[str]) -> str:
 prompt = f"""
{persona}

The customer said: "{customer_text}"
Relevant factual information:
{json.dumps(answers, indent=2)}

Combine these facts into a single short, natural response.
Use professional, empathetic language as a solar energy consultant.
Keep answers concise (1–3 sentences) and confident.
"""
 try:
     response = requests.post(
         "http://localhost:11434/api/generate",
         json={"model": "mistral", "prompt": prompt, "stream": False},
         timeout=30
     )
     data = response.json()
     return data.get("response", "").strip()
 except Exception as e:
     return f"(LLM unavailable: {e})"
Threading
Calls to generate_response() should run in a background thread to avoid blocking the UI.

Maintain a use_llm flag in main.py to toggle LLM on/off for A/B performance testing.

5. Data Flow Example
ASR Output:
"how much does solar cost and what happens on cloudy days"

Matcher Results:

bash
Copy code
[
  "The cost depends on system size and local incentives, but most residential solar systems range from $15,000 to $25,000 before tax credits.",
  "Yes, panels still generate power on cloudy days, though output may drop by 10 to 25 percent."
]
LLM Prompt (generated internally):
Combines persona.txt + customer text + matched answers → sends to Ollama.

LLM Output (to UI):
"Most homeowners spend about what they already pay for power, then lock that rate in long-term. Even on cloudy days, panels still generate enough to keep your system productive."

6. Persona Reference
File: persona.txt

Always load this at startup into memory.

It defines tone, empathy level, and communication style (professional, non-pushy, Chris Voss inspired).

The LLM prompt should prepend persona content before any question or factual data.

7. Fallback Behavior
If the LLM call fails or use_llm = False:

Revert to previous direct Q&A response display.

Log the exception but continue processing normally.

8. Performance and Logging
Time each LLM call (start → end) and print to console:
"LLM response time: 0.84 s".

Compare total cycle time with and without LLM enabled.

Ensure the application remains functional even if Ollama is not running.

9. Testing Plan
Enable use_llm = False → confirm baseline still works.

Enable use_llm = True → measure average response latency.

Ask multi-part questions (“How much does solar cost and what about cloudy days?”).

Confirm stacked responses merge naturally.

Adjust persona wording if tone feels off or responses are too long.

10. Deliverables Summary
New file: llm.py (handles all Ollama calls)

Updated: main.py (adds LLM toggle + thread call)

Updated: requirements.txt (add requests)

Existing: persona.txt, qa_script.json reused unchanged

Optional: Add performance_log.txt for response times.

11. Success Criteria
LLM output consistently < 1.5 seconds on local hardware.

Responses sound natural, aligned with persona tone.

Application operates fully offline.

Toggle between LLM and direct-matcher modes without breaking workflow.