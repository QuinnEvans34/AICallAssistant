# Project: CallAssist-LLM (Live Streaming Upgrade)
# Objective:
Convert the current turn-based (silence-triggered) architecture into a continuous, real-time speech streaming system.
The app should process live microphone or VOIP input, transcribe audio incrementally, detect complete questions as they occur, and display LLM-generated responses dynamically without waiting for long pauses.

---

## 1. Overview

The current system waits for user silence to finalize transcription before matching and responding.
This causes latency and mid-sentence cut-offs.

The new system must:
1. Capture and transcribe audio continuously.
2. Process speech in overlapping chunks (~0.5–1 s).
3. Stream partial text to the UI as it’s recognized.
4. Detect complete questions in the running transcript.
5. Send completed questions immediately to the matcher + LLM pipeline.
6. Display responses live in real-time.
7. Maintain smooth conversation flow without waiting for end-of-speech.

---

## 2. Architectural Summary

Audio (VOIP stream)
↓
Streaming Recorder (0.5 s chunks)
↓
Whisper (incremental transcriptions, 0.75 s overlap)
↓
Question Detector (regex / WH-word / ? / conjunctions)
↓
Queue → Matcher (find factual answers)
↓
Queue → LLM (Ollama Mistral 7B)
↓
Live Response Queue → UI display

yaml
Copy code

Multiple background threads communicate via queues to maintain continuous, non-blocking flow.

---

## 3. New/Modified Modules

### 3.1 `stream_asr.py` (replaces silence-based ASR)

Responsibilities:
- Record continuously from VOIP or microphone input.
- Segment audio into ~0.5 s overlapping chunks.
- Transcribe each chunk in a background thread using `faster-whisper`.
- Return partial text every 500–700 ms via callback.

Key configuration:
```python
CHUNK_SEC = 0.5
OVERLAP_SEC = 0.75
SAMPLE_RATE = 16000
Whisper call example:

python
Copy code
segments, _ = model.transcribe(audio_chunk, beam_size=1)
text = " ".join([seg.text for seg in segments]).strip()
callback(text)
Partial text should be merged into a rolling transcript buffer.

3.2 question_detector.py
Purpose:
Identify full questions in the live transcript.

Algorithm outline:

Append each new partial transcription to a text buffer.

Search for question-ending patterns:

'?'

keywords: how, what, why, can, is, do, would, should, when, where

conjunction triggers: "and", "also", "what about"

When a question boundary is detected, extract that substring and push it into question_queue.

Keep partial/incomplete sentences in buffer until completed.

python
Copy code
if text.endswith("?") or "what" in text or "how" in text:
    complete_question = extract_question_segment(buffer)
    question_queue.put(complete_question)
3.3 response_manager.py
Purpose:
Consume new questions from question_queue, run matcher and LLM pipeline, then push formatted answers into response_queue.

Steps:

Pop question.

Run matcher.match(question) → get list of factual answers.

Send to llm.generate_response() (reuse existing Mistral API call).

Push formatted result into response_queue.

Handle multiple simultaneous questions gracefully by processing them in independent threads or an async loop.

3.4 ui_stream.py
Purpose:
Display partial transcriptions and live responses in real time.

Functions:

Continuously print/append new partials as they arrive.

Show answers under the corresponding question once ready.

Differentiate color or label (e.g., CUSTOMER / ASSISTANT).

Keep transcript scrollable without blocking new updates.

Example:

pgsql
Copy code
[Customer]: how much does solar cost and what happens on cloudy days
[System]: Most homeowners spend about what they already pay for power, then lock that rate in long-term. Even on cloudy days, panels still generate enough to keep your system productive.
4. Concurrency Model
Threads:

RecorderThread – captures audio, sends to ASR.

ASRThread – transcribes and pushes partial text.

DetectorThread – scans partial text for questions.

ResponseThread – matches + calls LLM.

UIThread – updates live display.

All queues (audio_q, text_q, question_q, response_q) are thread-safe and non-blocking.

5. Context Management
Maintain a rolling transcript window:

Keep last ~30 seconds of recognized text for context.

Discard older segments to prevent memory buildup.

Allow LLM to use last 2–3 customer questions for reference:

ini
Copy code
conversation_context = previous_questions[-3:]
Feed that into the LLM prompt:

python
Copy code
persona + current_question + context + matched_answers
This enables the model to provide follow-up continuity (“As we discussed earlier…”).

6. Stacking Responses (Multi-Question Handling)
If the Question Detector finds multiple questions in one utterance:

Extract each separately.

Run them through the matcher individually.

Combine all factual answers into a list and send together to the LLM.

Prompt example:

markdown
Copy code
The customer asked:
1. how much does solar cost
2. what happens on cloudy days
Relevant answers:
1. [answer1]
2. [answer2]
Combine these answers into a short, cohesive response.
7. LLM Integration (Ollama Mistral 7B)
Continue using llm.py from previous version.

Adjust prompt logic:

Include conversation_context for continuity.

Keep output short, confident, and in the same persona tone.

Maintain ~1–2 s latency budget.

API endpoint:

bash
Copy code
POST http://localhost:11434/api/generate
model: "mistral"
Handle timeouts and fallback to direct Q&A if Ollama unavailable.

8. Performance and Optimization Targets
Component	Goal	Notes
Whisper partial latency	< 0.7 s	tune chunk + overlap
LLM response latency	< 1.5 s	Mistral 7B via Ollama
End-to-end delay	< 2.2 s	continuous interaction
Memory footprint	< 8 GB	full pipeline on CPU/GPU
Error recovery	auto restart on ASR/LLM timeout	no crash

9. Testing Procedure
Run system with use_llm=False → ensure live transcript displays correctly.

Enable use_llm=True → verify responses stream in without freezing.

Speak multi-question phrases:

“How long does installation take and what about cloudy days?”

“Can I add a battery later, and how much does that cost?”

Confirm:

Partial text appears live.

Each question triggers stacked answers.

Responses appear within 1–2 s after each question.

10. Deliverables Summary
New: stream_asr.py – live ASR engine

New: question_detector.py – detects questions dynamically

New: response_manager.py – handles matcher + LLM pipeline

New: ui_stream.py – displays partials and answers live

Modified: main.py – orchestrates all threads and queues

Existing: llm.py, persona.txt, qa_script.json remain active

11. Success Criteria
The system produces partial transcriptions continuously.

Responses appear live without waiting for silence.

Multiple questions per turn trigger stacked answers.

Latency feels “instantaneous” (< 2 s to first response).

The application runs fully offline using Ollama Mistral 7B.

No loss of context or mid-sentence cut-offs occur.